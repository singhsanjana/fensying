\begin{table}
		\input{section/figures-tables/results_table_1.tex}
	\caption{Bare bone results}
	\label{tab:results1}
\end{table}

In order to analyze and evaluate the performance of the algorithm 
proposed in this paper, we created a tool called ----. 77 benchmarks were
executed on it, using a computer with \ishComment{specs}.
Most of these benchmarks have been acquired from verification tool
repositories, namely CDSChecker \cite{cds}, Tracer \cite{tracer2018},
GenMC \cite{genmc-PLDI19}, RCMC \cite{rcmc-POPL18}, Watts \cite{watts}.
These benchmarks have been modified in terms of format to replicate the
one required by CDSChecker's input programs.
Some programs were translated into C11 and suitable
memory orders were added where no memory orders were specified.

All benchmarks can be segregated into two base classes. The first set consists of 
programs whose buggy executions can be stopped by inserting fences. Table \ref{tab:results1}
summarizes these results. The second category of benchmarks contains programs which
either do not provide obtainable results using a model checking tool, or
whose buggy behaviours cannot be stopped simply by using fences. \textit{\{The reason
for this may be because those programs are semantically created to prove
a point instead of being stopped using fences.\}} Benchmarks range from
having a few buggy executions to having thousands of buggy executions. They
also vary in the number of threads and atomic instructions in each of
their traces.

For each concurrent program, we provide the following information as criteria for evaluation:
the number of buggy executions found by the model checker, the number of fences
inserted in the final program, and the average number of atomic instructions
in each trace. The tool is assessed by being timed according to different steps
of the program, including time taken by the model checker, time taken by the
SAT solver and the computation time taken by the tool itself. Programs
were run in two modes- the regular mode with all executions
being taken into account and the \texttt{-t} flag mode, where only the first
trace was taken into consideration for each iteration of the input program.

\begin{figure}
	a. $O(n^3)$, \\
	\textit{where n is the number of vertices}
	
	
	b. $O((n+e)(c+1))$, \\
	\textit{for n nodes, e edges and c elementary circuits}
	\caption{a. complexity of computing transitive \lhb relations\\
	b. complexity of Johnson's algorithm to compute all elementary cycles in a graph}
\end{figure}

Through experiments, it was also observed that the two most expensive 
operations in the entire process were computing transitive relations for \lhb
relations, and computing all possible elementary cycles in the graph of \lto
relations. The metric, ``number of instructions in each trace'' adds
considerably to the former factor. A large number of instructions in each
trace produces a greater number of \lhb relations, thereby resulting in
greater computation time for transitive relations. This is evidenced by the benchmarks
\texttt{publish-sc0} and \texttt{publish-sc1} where the number of instructions in each
execution trace is 55 - a greater than average amount for our benchmarks.
Consequently, the tool takes more time solving them.

This metric does not necessarily apply to \lto relations.
A greater number of instructions may or may not produce a sizeable \lto graph.
This depends upon the semantics of the input program. However, chances are,
an intricate graph with many \lto relations will result in an enormous number of cycles, 
sometimes having exponential numbers, resulting in a much greater
computation time. This is evidenced by looking at the \z computation 
time column, since the SAT Solver is only concerned with reducing the cycles
to a minimal required number of fences. For instance, the benchmark \texttt{pgsql0}, though having 
a very small number of buggy executions, has an execution time of a few minutes. Much
of this time in spent on SAT Solving, as can be witnessed. This result reveals 
a very large number of \lto relations and hence cycles.

\begin{table}
\begin{center}
	\input{section/figures-tables/results_table_2.tex}
	\caption{Results after adding time-optimization}
	\label{tab:results2}
\end{center}
\end{table}

To combat this problem of number of cycles or number of buggy executions going out of bounds,
the optimization discussed in \ref{sec:opti} was applied.
Table \ref{tab:results2} contains a few of these results. Results state that
in most cases, the number of fences remains the same as they were with the 
full optimization, but the time is considerably reduced.

The tool performance cannot be compared with any previous approaches such as
the ones seen in \cite{ref1,ref2,etc}. This is mainly due to the fact that
no previous approaches for fence synthesis have been for the C11 model. Apart
from the model semantics and the memory models being completely different,
C11 fences are incomparable as well. Compared to regular fences, C11 fences have
different behaviours and semantics.

